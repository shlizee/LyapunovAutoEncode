# -*- coding: utf-8 -*-
"""Test Set Prep.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tNFk3F5k4DqLjqmVOFwxDeMgfmF79d3b
"""

from models import RNNModel
import torch
from config import *
import glob
from tqdm import tqdm

def process_text(text):
    test_chars= set([c for c in text])
    for c in test_chars:
        if c not in dcon.datasets['char_to_int'].keys():
            text = text.replace(c, '')
    data = torch.ByteTensor(len(text))
    for idx, c in enumerate(text):
        data[idx] = char_map[c]
    return data

if __name__ == '__main__':
	device = torch.device('cpu')
	dcon = torch.load('dcon.p', map_location = torch.device('cpu'))
	mcon = ModelConfig('lstm', 1, 64, 82,output_size = 82,dropout=0.1,
							init_type = 'uni', device = device, bias = False, id_init_param = 'b', encoding = 'one_hot')
	model = RNNModel(mcon)

	seq_length = 100
	batch_size = 32
	full_input_set = torch.ByteTensor(0, batch_size, seq_length - 1)
	full_target_set = torch.ByteTensor(0, batch_size, seq_length - 1)
	char_map = dcon.datasets['char_to_int']

	txt_dir = 'data/'
	text_files = glob.glob(txt_dir+ "*.txt")
	for file in tqdm(text_files):
		f = open(file, 'rt', encoding = 'utf8')
		orig_text = f.read()
		text = orig_text
	#     print(orig_text.split('}} \n \n'))
	#     text = orig_text.split('}} \n \n')[1]

		data = process_text(text)
		target = torch.ByteTensor(len(data))
		for i in range(int(len(data) / seq_length)):
			src_seq = data[i * seq_length : (i + 1) * seq_length]   # length: seq_length
			dest_seq = src_seq.clone()
			dest_seq[0:-1] = src_seq[1:]
			# dest_seq[-1] = src_seq[0]
			target[i * seq_length : (i + 1) * seq_length] = dest_seq

		batches = math.floor((len(data) / (batch_size * seq_length)))
		train_idx = 0   # start point of train set
		input_set = torch.LongTensor(batches, batch_size, seq_length - 1)
		target_set = torch.LongTensor(batches, batch_size, seq_length - 1)
		for i in range(batches):
			for j in range(batch_size):
				start_idx = train_idx + i * (batch_size * seq_length) + j * seq_length
				end_idx = start_idx + seq_length - 1
				input_set[i][j] = data[start_idx : end_idx]
				target_set[i][j] = target[start_idx: end_idx]
	#     print(input_set.shape)
		full_input_set = torch.cat((full_input_set, input_set), dim = 0)
		full_target_set = torch.cat((full_target_set, target_set), dim = 0)

	samples = full_target_set.shape[0]
	split_idx = math.floor(samples*0.8)
	shuffle_idx = torch.randperm(full_target_set.shape[0])
	train_set = (full_input_set[shuffle_idx][:split_idx], full_target_set[shuffle_idx][:split_idx])
	val_set = (full_input_set[shuffle_idx][split_idx:], full_target_set[shuffle_idx][split_idx:])
	data = {'train_set': train_set, 'val_set': val_set, 'char_to_int': char_map}

	torch.save(data, 'data/book_data.p')